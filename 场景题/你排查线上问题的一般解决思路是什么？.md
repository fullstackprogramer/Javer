# 你排查线上问题的一般解决思路是什么？

案例 1
1.查看线上日志：定位问题发生代码的版本（如果 maven 有用 git-plugin 很容易就能定位到 commitid）、问题
代码行数、问题参数（log 中有的话）
 2.回到代码中，分析出现的场景，复现问题，到这里一般能解决百分八十的问题了
 3.问题复现难道大点的，可能需要特定的数据场景支持了，想方法回溯数据场景：通过出现问题的时间，来找到关联服务的 log、中间价的 log、mysqlbinlog
 4.如果测试环境复现不了，但线上问题一直存在而非偶现：将问题的个别服务器从集群中暂时隔离下线但不停止服务，然后分析这台服务器。首先通过 jmap 将内存 dump 到本地查看内存变量情况、调用链情况，分析是否特定参数、特定并发场景下出现。如果一帧的内存还不能很好的分析，就远程端口接入远程调试。
案例2：
1 确认当前运行的 jar 文件 gitcommitid 必要时反编译 class 文件，确诊代码版本，如果是代码版本有问题及热修复或者回滚
 2查看日志报错信息，确认是否是代码逻辑导致的异常
 3 检查数据库锁，CPU.磁盘网络 io.jstack 查看线程阻塞情况，jmap 查看内存信息
 4 对于一些因为复杂业务极端情况使用 artharsttwatchtrace 等命令组合使用定位到方法参数，返回值抛出异常，整个调用栈
 5tcpdump 抓包，dump 出tcp 层面发送的数据确认网络的问题
 事故：在一次发布版本后 netyy 和一大部分物联网设备连接不上，netty 日志显示设备反复断开连接重新建立连接，立即回滚版本，无效！线程，数据库，redis 等中间件正常，怀疑是因为 netty 重启后短时间内设备大量重连服务器压力过大，对频繁调用的可能存在性能问题的方法 arthas 调用计时一切真常但是 cpu 等信息一切正常。极端措施一台服务器只连接一台故障设备，依旧连接不上。可以确定硬件是没有更新固件的。log 显示是设备端主动断开连接的，设备端的日志是服务器主动断开连接的。tcpdump 出来四次挥手确实是设备主动断开连接的，石锤！但是设备端很无辜的说我们什么的都没改怎么可能是我们的问题。最后对 tcpdump 出来的包一个一个仔细查看，终于发现了一点不同，到抓到的包并不会导致设备反复重连，最终抽丝剥茧，每一个细节都不放过终于通过三行代码修复了一个及其严重的但又毫无技术难度可言的 bug
案例3
调用访问慢和失败？
排查
一、传输层
1.传输层的配置，tcp 的连接总数 filemax 是否上限(netstat可统计当前连接数多少)；
2.tcp 连接中是否有许多死连接(client端再四步挥手或其它情况断开网络数据就会造成死连接)；tcp 具体实现是有keepalive机制，默认 7200 秒(约两小时)检查一次，可以调整；
应用自身也可以实现 maxIdle(最大空闲)机制，可参考著名的 netty,应用自身实现可以额外做一下采集指标或其它事件处理
3.网络层(ICMP 协议)的实现 ping 做下侦测工作 RTT 耗时，traceroute 做下网络包的经过路径(TTL经过一级自身减一，默认大多 64，最大值一个无符号字节255)
4.服务器带宽是否到达峰值，这个通过采集 linux 的流量统计的文件(unit:bytes),；还需要采集应用自身的进出口流量(应用自身实现流量统计)等来做出判定
……
二、应用层(网络 io 型场景)
1.业务逻辑实现层需要加filterbefore=>业务逻辑层=>filterafter需要实现一个耗时记录并且采集记录耗时任务(缩小问题范围)和相关元数据(具体 api代称)
2.read 和 write 不成对比时，缓冲区就会无限扩大(任务队列体积)，这时候就需要断流，停止调用 socketread()函数，等任务队列达到最小求任务再开始调用socketread()函数，这时候架构没有问题就需要加机器
3.内存泄露的大问题特别不出名的第三方开源库，通过 jmap 生成的 HeapDump 文件，去分析实例数量和 bytes……所以最好开发能写出对应测试工具(开发人员最清楚可能出现几个问题范围和指标)



> 原文: <https://www.yuque.com/tulingzhouyu/db22bv/rknpslxux50gi3of>